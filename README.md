<div align="center">
  <img src="https://github.com/user-attachments/assets/ccb6f5f1-0e07-4eb2-aa7c-5f681c57a59c" alt="Descri√ß√£o da imagem" width="1000"/>
</div>

<h1 align="center">Tarrasque üßå</h1>

<h2 align="center">Previs√£o de severidade de enfisema</h2>

<h4 align="center"><strong>Autores:</strong> J√∫lia Guedes, Lorena Ribeiro e Emily Gomes</h4>

<h4 align="center"><strong>Professor:</strong> Daniel R. Cassar</h4>


## üìù Descri√ß√£o
<p align="justify"> 
De forma geral, √© poss√≠vel definir otimiza√ß√£o como uma abordagem matem√°tica voltada para a busca da melhor solu√ß√£o poss√≠vel dentro de um conjunto de alternativas. Sendo que, para isso, √© preciso respeitar os objetivos e restri√ß√µes impostos pelo problema. Atualmente, diversas estrat√©gias de otimiza√ß√£o est√£o dispon√≠veis, sendo sua aplica√ß√£o dependente da natureza e complexidade do cen√°rio analisado [1]. No contexto do aprendizado de m√°quina, emergem 3 principais formas de otimiza√ß√£o de hiperpar√¢metros: Busca Aleat√≥ria, busca em grade e otimiza√ß√£o bayesiana (Optuna). Para esse trabalho, ser√£o considerados como hiperpar√¢metros, ser√£o considerados a fun√ß√£o de ativa√ß√£o, n√∫mero de camadas, quantidade de neur√¥nios por camada e taxa de aprendizado.
</p>

<p align="justify">
Esse reposit√≥rio busca explorar essas tr√™s formas de otimiza√ß√£o para a constru√ß√£o de uma rede neural <em> Multilayer Perceptron</em> (MLP) classificadora multiclasse. Para a constru√ß√£o do problema, o dataset escolhido foi o "CD4/CD8 Ratio: T helper cells", do m√≥dulo Kaggle, o qual trata sobre severidade de enfisemas pulmonares em pessoas infectadas com HIV. Como algumas <em>features</em>, temos caracter√≠sticas do pacientes analisados, tais como idade, g√™nero, ind√≠ce de massa corporal, uso de drogas e marcadores relacionados a essas doen√ßas presentes no sangue e no plasma, como CD14 sol√∫vel, volume expirat√≥rio for√ßado (FEV1) e capacidade de difus√£o (DLCO). 
</p>

## üìî Notebooks e arquivos do projeto
* `Tarrasque.zip`: Notebook principal que implementa os c√≥digos de otimiza√ß√µes com as diferentes t√©cnicas, bem como aprofunda os conceitos te√≥ricos envolvidos e analisa os resultados obtidos por cada tipo de otimiza√ß√£o.
* `imagens`: Cont√©m a logo utilizada no Notebook principal e no READ ME do Github ("logo_ilum-CNPEM.png")

## ü™º M√©todos para a busca de hiperpar√¢metros
**Busca aleat√≥ria**
<p align="justify">
A <strong>busca aleat√≥ria</strong>, por sua vez, utiliza distribui√ß√µes estat√≠sticas em vez de valores discretos para os par√¢metros. Ao longo das itera√ß√µes, os modelos s√£o comparados com o objetivo de encontrar a melhor configura√ß√£o. Nessa metodologia, n√£o h√° garantia de que o melhor modelo ser√° encontrado, mas, geralmente, ela retorna resultados compar√°veis aos da busca em grade, com menor tempo de execu√ß√£o. [2]</p>
  
<p align="justify">
Para a realiza√ß√£o desse tipo de busca, o m√≥dulo ``Optuna``, definido no modo aleat√≥rio ``(sampler=optuna.samplers.RandomSampler(seed=51012))``, foi implementado. A partir do resultado da melhor arquitetura, essa foi testada para a rede MLP, em que dentre as an√°lises foi apresentada a seguinte matriz de confus√£o:</p>

<p align="justify">
  <img src="https://github.com/user-attachments/assets/f100eb94-d1b2-48d3-95d8-cb9172f67e23" alt="image" style="display: block; margin: auto; text-align: justify;">
</p>


**Busca em grade**
<p align="justify">
A <strong>busca em grade</strong> √© uma metodologia para o ajuste de hiperpar√¢metros que consiste em explorar exaustivamente o espa√ßo com todos os conjuntos poss√≠veis de hiperpar√¢metros. Com isso, o objetivo √© encontrar o melhor conjunto dessas vari√°veis para o modelo. Contudo, ao gerar todas as configura√ß√µes poss√≠veis de valores discretos, h√° um alto consumo de recursos computacionais e, desse modo, essa abordagem mostra-se ineficiente para lidar com a maioria dos problemas. [2]

<p align="justify">
Para a defini√ß√£o desse tipo de busca, o m√≥dulo `GridSearchCV` da biblioteca ``Scikit-Learn`` foi utilizado. A partir da arquitetura com melhor desempenho encontrado, essa foi aplicada posteriormente na rede, avaliando sua acur√°cia, e gerando a matriz de confus√£o desse modelo:

![image](https://github.com/user-attachments/assets/e857ecdc-6cbd-4ea2-b630-81491dcb99bd)

</p>

**Otimiza√ß√£o bayesiana**
<p style="text-align:justify;">
Em rela√ß√£o ao <strong>Optuna</strong>, esse algoritmo utiliza o princ√≠pio do Teorema de Bayes para encontrar os hiperpar√¢metros. Ou seja, o processo √© iterativo, sendo que o pr√≥ximo palpite para a combina√ß√£o de vari√°veis depende da anterior. A partir disso, s√£o selecionados de forma probabil√≠stica um novo conjunto de valores de hiperpar√¢metros com maior probabilidade de gerar melhores resultados. 
</p>

<p align="justify">
Finalmente, em rela√ß√£o a essa forma de otimiza√ß√£o, o modo cl√°ssico do m√≥dulo ``Optuna`` foi utilizado. Gerando a melhor arquitetura de rede encontrada, que foi testada na MLP classificadora, resultando na matriz de confus√£o apresentada a seguir:
</p>

![image](https://github.com/user-attachments/assets/ba9da3be-ec97-4f31-a251-2703c011fb9d)


## üòÅ Conclus√£o

<p align="justify">
Ao final do projeto, foi poss√≠vel otimizar os par√¢metros de uma rede MLP classificadora multiclasse, a qual visava a previs√£o da severidade de enfisemas pulmonares em pacientes com HIV. A partir dos tr√™s m√©todos de otimiza√ß√£o apicados, foi poss√≠vel obter tr√™s conjuntos de hiperpar√¢metros. Vale ressaltar que os hiperpar√¢metros dos m√©todos de busca em grade e busca bayesiana foram definidos dentro de um intervalo poss√≠vel, enquanto a busca em grade foi feita com valores de hiperpar√¢metros definidos de forma discreta e sem dropout. A partir do treinamento e teste das redes MLP com cada uma das combina√ß√µes de par√¢metros, obteve-se diferentes valores de acur√°cia, sendo os dois maiores valores de 41.18% com a busca aleat√≥ria e bayesiana. Quanto ao menor resultado, foi a acur√°cia de 35.29%, com a busca em grade. Quanto a matriz de confus√£o tanto a busca em grade, quanto busca aleat√≥ria apresentam apenas dois valores na diagonal principal, enquanto a busca bayesiana apresenta tr√™s. Vale ressaltar tamb√©m que em todas, as predi√ß√µes mais fidedignas foram em rela√ß√£o a classe 0, que apresenta maior frequ√™ncia de dados. Apesar dos valores iguais entre a acur√°cia do modelo Bayesiano e de Busca aleat√≥ria, as matrizes de confus√£o foram diferentes o que demonstra modelos diferentes em suas predi√ß√µes.  Portanto, a acur√°cia pr√≥xima ao baseline, pode ter ocorrido devido ao desbalanceamento das classes. Al√©m disso, foi poss√≠vel observar que a aplica√ß√£o da ferramenta de parada antecipada foi uma ferramenta que auxilixou na redu√ß√£o do custo computacional .Desse modo, foi poss√≠vel testar m√©todos diferentes de otimizadores e avaliar o desempenho de cada um deles para a predi√ß√£o desse modelo. 
</p>

## üñáÔ∏è Informa√ß√µes t√©cnicas
* Linguagem de programa√ß√£o: `Python 3.9`
* Software:  `Jupyter Notebook`

* Bibliotecas e M√≥dulos: `random` `zipfile` `os` `pandas` `scikit-learn` `skorch` `pandas` `numpy` `itertools` `torch` `optuna` `copy`
<br>


## üë©‚Äçü¶≥ Refer√™ncias
[1] [What Is Optimization Modeling? | IBM](https://www.ibm.com/think/topics/optimization-model). Acesso em: 4 maio 2025.

[2] [What Is Hyperparameter Tuning? | IBM](https://www.ibm.com/think/topics/hyperparameter-tuning). Acesso em: 7 maio 2025.

[3] [SKORCH ‚Äì Quickstart: Grid Search](https://skorch.readthedocs.io/en/stable/user/quickstart.html#grid-search). Acesso em: 3 jun. 2025.

[4] [Quickstart ‚Äî skorch 1.1.0 documentation](https://skorch.readthedocs.io/en/stable/user/quickstart.html#grid-search). Acesso em: 3 jun. 2025.

[5] [Tuning MLP by using Optuna ‚Äì Gist by toshihikoyanase](https://gist.github.com/toshihikoyanase/7e75b054395ec0a6dbb144a300862f60). Acesso em: 28 mai. 2025.

[6] [scikit-learn hyperparameter optimization for MLPClassifier ‚Äì PANJEH (Medium)](https://panjeh.medium.com/scikit-learn-hyperparameter-optimization-for-mlpclassifier-4d670413042b). Acesso em: 21 mai. 2025.

 ## üß† Contribui√ß√µes dos Colaboradores
| GitHub | Contribui√ß√µes |
|:-----|:--------------|
| [J√∫lia Guedes A. dos Santos](https://github.com/JuliaGuedesASantos) | Introdu√ß√£o, otimiza√ß√£o de modelos (bayesiano e busca aleat√≥ria), treinamento final dos modelos, coment√°rios no c√≥digo e READ ME |
| [Lorena Ribeiro Nascimento](https://github.com/lorena881) | Introdu√ß√£o, otimiza√ß√£o de modelos (bayesiano e busca aleat√≥ria), coment√°rios no c√≥digo, READ ME | (https://github.com/MEmilyGomes)
| [Maria Emily Nayla Gomes da Silva](https://github.com/lorena881) | Introdu√ß√£o, otimiza√ß√£o de modelos (busca em grade), READ ME e coment√°rios no c√≥digo |
| [Daniel Roberto Cassar](https://github.com/drcassar) | Orientador |
 
