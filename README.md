<div align="center">
  <img src="https://github.com/user-attachments/assets/ccb6f5f1-0e07-4eb2-aa7c-5f681c57a59c" alt="Descri√ß√£o da imagem" width="1000"/>
</div>

<h1 align="center">Tarrasque üßå</h1>

<h2 align="center">Previs√£o de severidade de enfisema</h2>

<h4 align="center"><strong>Autores:</strong> J√∫lia Guedes, Lorena Ribeiro e Emily Gomes</h4>

<h4 align="center"><strong>Professor:</strong> Daniel R. Cassar</h4>

<p align="center">
<img loading="lazy" src="http://img.shields.io/static/v1?label=STATUS&message=EM%20DESENVOLVIMENTO&color=GREEN&style=for-the-badge"/>
</p>

## üìù Descri√ß√£o
<p align="justify"> 
De forma geral, √© poss√≠vel definir otimiza√ß√£o como uma abordagem matem√°tica voltada para a busca da melhor solu√ß√£o poss√≠vel dentro de um conjunto de alternativas. Sendo que, para isso, √© preciso respeitar os objetivos e restri√ß√µes impostos pelo problema. Atualmente, diversas estrat√©gias de otimiza√ß√£o est√£o dispon√≠veis, sendo sua aplica√ß√£o dependente da natureza e complexidade do cen√°rio analisado [1]. No contexto do aprendizado de m√°quina, emergem 3 principais formas de otimiza√ß√£o de hiperpar√¢metros: Busca Aleat√≥ria, busca em grade e otimiza√ß√£o bayesiana (Optuna). Para esse trabalho, ser√£o considerados como hiperpar√¢metros, ser√£o considerados a fun√ß√£o de ativa√ß√£o, n√∫mero de camadas, quantidade de neur√¥nios por camada e taxa de aprendizado.
</p>

<p align="justify">
Esse reposit√≥rio busca explorar essas tr√™s formas de otimiza√ß√£o para a constru√ß√£o de uma rede neural <em> Multilayer Perceptron</em> (MLP) classificadora multiclasse. Para a constru√ß√£o do problema, o dataset escolhido foi o "CD4/CD8 Ratio: T helper cells", do m√≥dulo Kaggle, o qual trata sobre severidade de enfisemas pulmonares em pessoas infectadas com HIV. Como algumas <em>features</em>, temos caracter√≠sticas do pacientes analisados, tais como idade, g√™nero, ind√≠ce de massa corporal, uso de drogas e marcadores relacionados a essas doen√ßas presentes no sangue e no plasma, como CD14 sol√∫vel, volume expirat√≥rio for√ßado (FEV1) e capacidade de difus√£o (DLCO). 
</p>

## üìî Notebooks e arquivos do projeto
* `ATP -5.1 Tiamat.ipynb`: Notebook principal que implementa os c√≥digos de otimiza√ß√µes com as diferentes t√©cnicas, bem como aprofunda os conceitos te√≥ricos envolvidos e analisa os resultados obtidos por cada tipo de otimiza√ß√£o.
* `imagens`: Cont√©m a logo utilizada no Notebook principal e no READ ME do Github ("logo_ilum-CNPEM.png")

## ü™º M√©todos para a busca de hiperpar√¢metros
**Busca aleat√≥ria**
<p align="justify">
A <strong>busca aleat√≥ria</strong>, por sua vez, utiliza distribui√ß√µes estat√≠sticas em vez de valores discretos para os par√¢metros. Ao longo das itera√ß√µes, os modelos s√£o comparados com o objetivo de encontrar a melhor configura√ß√£o. Nessa metodologia, n√£o h√° garantia de que o melhor modelo ser√° encontrado, mas, geralmente, ela retorna resultados compar√°veis aos da busca em grade, com menor tempo de execu√ß√£o. [2]
</p>

Para a realiza√ß√£o desse tipo de busca, o m√≥dulo ``Optuna``, definido no modo aleat√≥rio ``(sampler=optuna.samplers.RandomSampler(seed=51012))``, foi utilizado, a partir do seguinte c√≥digo:

````python

from sklearn.model_selection import GridSearchCV


class MyModule(nn.Module):
    def __init__(self, num_units=10, nonlin=nn.ReLU(), num_layers=2):
        super().__init__()
        self.nonlin = nonlin

        layers = []
        input_size = 30  # n√∫mero de features do X
        for i in range(num_layers):
            layers.append(nn.Linear(input_size, num_units))
            layers.append(nonlin)
            input_size = num_units  # as pr√≥ximas camadas recebem `num_units`

        self.hidden = nn.Sequential(*layers)
        self.output = nn.Linear(num_units, QUANTIDADE_TARGET)

    def forward(self, X, **kwargs):
        X = self.hidden(X)
        X = self.output(X)
        return X
    
net = NeuralNetClassifier(
    MyModule,
    max_epochs=100,
    criterion=nn.CrossEntropyLoss(),
    lr=0.1,
    # Shuffle training data on each epoch
    iterator_train__shuffle=True,
)
    
params = {
    'lr': TAXA_APRENDIZADO,
    'module__num_units': NEURONIOS,         # por exemplo [10, 20, 50]
    'module__nonlin': [nn.ReLU(), nn.Sigmoid()],
    'module__num_layers': [1, 2, 3, 4]       # experimenta diferentes profundidades
}
gs = GridSearchCV(net, params, refit=False, cv=3, scoring='accuracy')

X_treino = X_treino.astype(np.float32) #?
y_treino = y_treino.astype(np.int64)
gs.fit(X_treino, y_treino)
print(gs.best_score_, gs.best_params_)
````


**Busca em grade**
<p align="justify">
A <strong>busca em grade</strong> √© uma metodologia para o ajuste de hiperpar√¢metros que consiste em explorar exaustivamente o espa√ßo com todos os conjuntos poss√≠veis de hiperpar√¢metros. Com isso, o objetivo √© encontrar o melhor conjunto dessas vari√°veis para o modelo. Contudo, ao gerar todas as configura√ß√µes poss√≠veis de valores discretos, h√° um alto consumo de recursos computacionais e, desse modo, essa abordagem mostra-se ineficiente para lidar com a maioria dos problemas. [2]

Para a defini√ß√£o desse tipo de busca, o m√≥dulo `GridSearchCV` da biblioteca ``Scikit-Learn`` foi utilizado, seguindo o pipeline abaixo:

````python

from sklearn.model_selection import GridSearchCV


class MyModule(nn.Module):
    def __init__(self, num_units=10, nonlin=nn.ReLU(), num_layers=2):
        super().__init__()
        self.nonlin = nonlin

        layers = []
        input_size = 30  # n√∫mero de features do X
        for i in range(num_layers):
            layers.append(nn.Linear(input_size, num_units))
            layers.append(nonlin)
            input_size = num_units  # as pr√≥ximas camadas recebem `num_units`

        self.hidden = nn.Sequential(*layers)
        self.output = nn.Linear(num_units, QUANTIDADE_TARGET)

    def forward(self, X, **kwargs):
        X = self.hidden(X)
        X = self.output(X)
        return X
    
net = NeuralNetClassifier(
    MyModule,
    max_epochs=100,
    criterion=nn.CrossEntropyLoss(),
    lr=0.1,
    # Shuffle training data on each epoch
    iterator_train__shuffle=True,
)
    
params = {
    'lr': TAXA_APRENDIZADO,
    'module__num_units': NEURONIOS,         # por exemplo [10, 20, 50]
    'module__nonlin': [nn.ReLU(), nn.Sigmoid()],
    'module__num_layers': [1, 2, 3, 4]       # experimenta diferentes profundidades
}
gs = GridSearchCV(net, params, refit=False, cv=3, scoring='accuracy')

X_treino = X_treino.astype(np.float32) #?
y_treino = y_treino.astype(np.int64)
gs.fit(X_treino, y_treino)
print(gs.best_score_, gs.best_params_)
````

</p>

**Otimiza√ß√£o bayesiana**
<p style="text-align:justify;">
Em rela√ß√£o ao <strong>Optuna</strong>, esse algoritmo utiliza o princ√≠pio do Teorema de Bayes para encontrar os hiperpar√¢metros. Ou seja, o processo √© iterativo, sendo que o pr√≥ximo palpite para a combina√ß√£o de vari√°veis depende da anterior. A partir disso, s√£o selecionados de forma probabil√≠stica um novo conjunto de valores de hiperpar√¢metros com maior probabilidade de gerar melhores resultados. 
</p>

Finalmente, em rela√ß√£o a essa forma de otimiza√ß√£o, o modo cl√°ssico do m√≥dulo ``Optuna`` foi utilizado:
</p>

````python

from sklearn.model_selection import GridSearchCV


class MyModule(nn.Module):
    def __init__(self, num_units=10, nonlin=nn.ReLU(), num_layers=2):
        super().__init__()
        self.nonlin = nonlin

        layers = []
        input_size = 30  # n√∫mero de features do X
        for i in range(num_layers):
            layers.append(nn.Linear(input_size, num_units))
            layers.append(nonlin)
            input_size = num_units  # as pr√≥ximas camadas recebem `num_units`

        self.hidden = nn.Sequential(*layers)
        self.output = nn.Linear(num_units, QUANTIDADE_TARGET)

    def forward(self, X, **kwargs):
        X = self.hidden(X)
        X = self.output(X)
        return X
    
net = NeuralNetClassifier(
    MyModule,
    max_epochs=100,
    criterion=nn.CrossEntropyLoss(),
    lr=0.1,
    # Shuffle training data on each epoch
    iterator_train__shuffle=True,
)
    
params = {
    'lr': TAXA_APRENDIZADO,
    'module__num_units': NEURONIOS,         # por exemplo [10, 20, 50]
    'module__nonlin': [nn.ReLU(), nn.Sigmoid()],
    'module__num_layers': [1, 2, 3, 4]       # experimenta diferentes profundidades
}
gs = GridSearchCV(net, params, refit=False, cv=3, scoring='accuracy')

X_treino = X_treino.astype(np.float32) #?
y_treino = y_treino.astype(np.int64)
gs.fit(X_treino, y_treino)
print(gs.best_score_, gs.best_params_)

````


## üòÅ Conclus√£o

## üñáÔ∏è Informa√ß√µes t√©cnicas
* Linguagem de programa√ß√£o: `Python 3.9`
* Software:  `Jupyter Notebook`

* Bibliotecas e M√≥dulos: `random` `zipfile` `os` `pandas` `scikit-learn` `skorch` `pandas` `numpy` `itertools` `torch` `optuna` `copy`
<br>


## üë©‚Äçü¶≥ Refer√™ncias
[1] [What Is Optimization Modeling? | IBM](https://www.ibm.com/think/topics/optimization-model). Acesso em: 4 maio 2025.

[2] [What Is Hyperparameter Tuning? | IBM](https://www.ibm.com/think/topics/hyperparameter-tuning). Acesso em: 7 maio 2025.

[3] [SKORCH ‚Äì Quickstart: Grid Search](https://skorch.readthedocs.io/en/stable/user/quickstart.html#grid-search). Acesso em: 3 jun. 2025.

[4] [Quickstart ‚Äî skorch 1.1.0 documentation](https://skorch.readthedocs.io/en/stable/user/quickstart.html#grid-search). Acesso em: 3 jun. 2025.

[5] [Tuning MLP by using Optuna ‚Äì Gist by toshihikoyanase](https://gist.github.com/toshihikoyanase/7e75b054395ec0a6dbb144a300862f60). Acesso em: 28 mai. 2025.

[6] [scikit-learn hyperparameter optimization for MLPClassifier ‚Äì PANJEH (Medium)](https://panjeh.medium.com/scikit-learn-hyperparameter-optimization-for-mlpclassifier-4d670413042b). Acesso em: 21 mai. 2025.

 ## üß† Contribui√ß√µes dos Colaboradores
| GitHub | Contribui√ß√µes |
|:-----|:--------------|
| [J√∫lia Guedes A. dos Santos](https://github.com/JuliaGuedesASantos) | Introdu√ß√£o, otimiza√ß√£o de modelos (bayesiano e busca aleat√≥ria), treinamento final dos modelos e READ ME |
| [Lorena Ribeiro Nascimento](https://github.com/lorena881) | Introdu√ß√£o, otimiza√ß√£o de modelos (bayesiano e busca aleat√≥ria), coment√°rios no c√≥digo | (https://github.com/MEmilyGomes)
| [Maria Emily Nayla Gomes da Silva](https://github.com/lorena881) | Introdu√ß√£o, otimiza√ß√£o de modelos (busca em grade), READ ME e coment√°rios no c√≥digo |
| [Daniel Roberto Cassar](https://github.com/drcassar) | Orientador |
 
