<div align="center">
  <img src="https://github.com/user-attachments/assets/ccb6f5f1-0e07-4eb2-aa7c-5f681c57a59c" alt="Descri√ß√£o da imagem" width="1000"/>
</div>

<h1 align="center">Tarrasque üßå</h1>

<h2 align="center">Previs√£o de severidade de enfisema</h2>

<h4 align="center"><strong>Autores:</strong> J√∫lia Guedes, Lorena Ribeiro e Emily Gomes</h4>

<h4 align="center"><strong>Professor:</strong> Daniel R. Cassar</h4>

<p align="center">
<img loading="lazy" src="http://img.shields.io/static/v1?label=STATUS&message=EM%20DESENVOLVIMENTO&color=GREEN&style=for-the-badge"/>
</p>

## üìù Descri√ß√£o
<p align="justify"> 
De forma geral, √© poss√≠vel definir otimiza√ß√£o como uma abordagem matem√°tica voltada para a busca da melhor solu√ß√£o poss√≠vel dentro de um conjunto de alternativas. Sendo que, para isso, √© preciso respeitar os objetivos e restri√ß√µes impostos pelo problema. Atualmente, diversas estrat√©gias de otimiza√ß√£o est√£o dispon√≠veis, sendo sua aplica√ß√£o dependente da natureza e complexidade do cen√°rio analisado [1]. No contexto do aprendizado de m√°quina, emergem 3 principais formas de otimiza√ß√£o de hiperpar√¢metros: Busca Aleat√≥ria, busca em grade e otimiza√ß√£o bayesiana (Optuna). Para esse trabalho, ser√£o considerados como hiperpar√¢metros, ser√£o considerados a fun√ß√£o de ativa√ß√£o, n√∫mero de camadas, quantidade de neur√¥nios por camada e taxa de aprendizado.
</p>

<p align="justify">
Esse reposit√≥rio busca explorar essas tr√™s formas de otimiza√ß√£o para a constru√ß√£o de uma rede neural <em> Multilayer Perceptron</em> (MLP) classificadora multiclasse. Para a constru√ß√£o do problema, o dataset escolhido foi o "CD4/CD8 Ratio: T helper cells", do m√≥dulo Kaggle, o qual trata sobre severidade de enfisemas pulmonares em pessoas infectadas com HIV. Como algumas <em>features</em>, temos caracter√≠sticas do pacientes analisados, tais como idade, g√™nero, ind√≠ce de massa corporal, uso de drogas e marcadores relacionados a essas doen√ßas presentes no sangue e no plasma, como CD14 sol√∫vel, volume expirat√≥rio for√ßado (FEV1) e capacidade de difus√£o (DLCO). 
</p>

## üìî Notebooks e arquivos do projeto
* `ATP -5.1 Tiamat.ipynb`: Notebook principal que implementa os c√≥digos de otimiza√ß√µes com as diferentes t√©cnicas, bem como aprofunda os conceitos te√≥ricos envolvidos e analisa os resultados obtidos por cada tipo de otimiza√ß√£o.
* `imagens`: Cont√©m a logo utilizada no Notebook principal e no READ ME do Github ("logo_ilum-CNPEM.png")

## ü™º M√©todos para a busca de hiperpar√¢metros
**Busca aleat√≥ria**
<p align="justify">
A <strong>busca aleat√≥ria</strong>, por sua vez, utiliza distribui√ß√µes estat√≠sticas em vez de valores discretos para os par√¢metros. Ao longo das itera√ß√µes, os modelos s√£o comparados com o objetivo de encontrar a melhor configura√ß√£o. Nessa metodologia, n√£o h√° garantia de que o melhor modelo ser√° encontrado, mas, geralmente, ela retorna resultados compar√°veis aos da busca em grade, com menor tempo de execu√ß√£o. [2]

Para a realiza√ß√£o desse tipo de busca, o m√≥dulo ``Optuna``, definido no modo aleat√≥rio ``(sampler=optuna.samplers.RandomSampler(seed=51012))``, foi utilizado, a partir do seguinte c√≥digo:

````python
#Criando a inst√¢ncia com os par√¢metros necess√°rios
def cria_instancia_mlp(trial):
     """Cria uma inst√¢ncia do modelo desejado (MLP)"""
     n_camadas = trial.suggest_int('n_layers', 1, 10)
     num_features = 30
     camadas = [num_features]
     for i in range(n_camadas):
        camadas.append(trial.suggest_int(f'n_units_{i}', 2, 15))
     camadas.append(5)
     nome_ativacao = trial.suggest_categorical("funcao_de_ativacao", ["Relu", "Sigmoide"])
     funcao_de_ativacao = nn.ReLU() if nome_ativacao == "Relu" else nn.Sigmoid()
     camadas_rede = []
     for i in range(len(camadas) - 2):
        camadas_rede.append(nn.Linear(camadas[i], camadas[i+1]))
        camadas_rede.append(funcao_de_ativacao)
        p = trial.suggest_float("dropout_l{}".format(i), 0.2, 0.5)
        camadas_rede.append(nn.Dropout(p))
     camadas_rede.append(nn.Linear(camadas[-2], camadas[-1]))
     camadas_rede.append(nn.LogSoftmax(dim=1))

# Classe para constru√ß√£o da MLP com os par√¢metros sorteados em cada √©poca
     
     class MLP(nn.Module):   
        def __init__(self):
            super().__init__()
            camadas_rede = []
            for i in range(len(camadas) - 2):
                camadas_rede.append(nn.Linear(camadas[i], camadas[i+1]))
                camadas_rede.append(funcao_de_ativacao)
            camadas_rede.append(nn.Linear(camadas[-2], camadas[-1]))
            self.rede_neural = nn.Sequential(*camadas_rede)
        def forward(self, X):
            return self.rede_neural(X)

     return NeuralNetClassifier(
        MLP,
        max_epochs=100,
        lr=trial.suggest_float("learning_rate", 1e-5, 1e-2, log=True),
        criterion=torch.nn.CrossEntropyLoss,
        optimizer=torch.optim.Adam,
    )         

#Fun√ß√£o objetivo para an√°lise do desempenho da rede gerada, com aplica√ß√£o da valida√ß√£o cruzada 
def funcao_objetivo(trial, X, y, num_folds):
    """Computa a Acur√°cia - com a utiliza√ß√£o de valida√ß√£o cruzada - para teste a efici√™ncia das inst√¢ncias geradas """
    modelo = cria_instancia_mlp(trial)
 
    metricas = cross_val_score(
            modelo,
            X,
            y,
            scoring="accuracy",
            cv=num_folds,
        )
    return -metricas.mean()
 
def funcao_objetivo_parcial(trial):
    "Fun√ß√£o objetivo que apenas possui como argumento o objeto trial"
    return funcao_objetivo(trial, X_treino, y_treino, 3)
 
    
estudo_mlp= optuna.create_study(
    sampler=optuna.samplers.RandomSampler(seed=51012),
    direction="minimize",
    study_name="mlp_otimizacao_optuna_busca_aleatoria_final",
    storage=f"sqlite:///mlp_otimizacao_optuna_busca_aleatoria_final.db",
    load_if_exists=True,
    
    )
 
estudo_mlp.optimize(funcao_objetivo_parcial, n_trials=100)
 
melhor_trial_mlp = estudo_mlp.best_trial
 
    
parametros_melhor_trial_mlp_aleatorio = melhor_trial_mlp.params
print(f"Par√¢metros do melhor trial: {parametros_melhor_trial_mlp_aleatorio}")
````
</p>

**Busca em grade**
<p align="justify">
A <strong>busca em grade</strong> √© uma metodologia para o ajuste de hiperpar√¢metros que consiste em explorar exaustivamente o espa√ßo com todos os conjuntos poss√≠veis de hiperpar√¢metros. Com isso, o objetivo √© encontrar o melhor conjunto dessas vari√°veis para o modelo. Contudo, ao gerar todas as configura√ß√µes poss√≠veis de valores discretos, h√° um alto consumo de recursos computacionais e, desse modo, essa abordagem mostra-se ineficiente para lidar com a maioria dos problemas. [2]

Para a defini√ß√£o desse tipo de busca, o m√≥dulo `GridSearchCV` da biblioteca ``Scikit-Learn`` foi utilizado, seguindo o pipeline abaixo:

````python

from sklearn.model_selection import GridSearchCV


class MyModule(nn.Module):
    def __init__(self, num_units=10, nonlin=nn.ReLU(), num_layers=2):
        super().__init__()
        self.nonlin = nonlin

        layers = []
        input_size = 30  # n√∫mero de features do X
        for i in range(num_layers):
            layers.append(nn.Linear(input_size, num_units))
            layers.append(nonlin)
            input_size = num_units  # as pr√≥ximas camadas recebem `num_units`

        self.hidden = nn.Sequential(*layers)
        self.output = nn.Linear(num_units, QUANTIDADE_TARGET)

    def forward(self, X, **kwargs):
        X = self.hidden(X)
        X = self.output(X)
        return X
    
net = NeuralNetClassifier(
    MyModule,
    max_epochs=100,
    criterion=nn.CrossEntropyLoss(),
    lr=0.1,
    # Shuffle training data on each epoch
    iterator_train__shuffle=True,
)
    
params = {
    'lr': TAXA_APRENDIZADO,
    'module__num_units': NEURONIOS,         # por exemplo [10, 20, 50]
    'module__nonlin': [nn.ReLU(), nn.Sigmoid()],
    'module__num_layers': [1, 2, 3, 4]       # experimenta diferentes profundidades
}
gs = GridSearchCV(net, params, refit=False, cv=3, scoring='accuracy')

X_treino = X_treino.astype(np.float32) #?
y_treino = y_treino.astype(np.int64)
gs.fit(X_treino, y_treino)
print(gs.best_score_, gs.best_params_)
````

</p>

**Otimiza√ß√£o bayesiana**
<p style="text-align:justify;">
Em rela√ß√£o ao <strong>Optuna</strong>, esse algoritmo utiliza o princ√≠pio do Teorema de Bayes para encontrar os hiperpar√¢metros. Ou seja, o processo √© iterativo, sendo que o pr√≥ximo palpite para a combina√ß√£o de vari√°veis depende da anterior. A partir disso, s√£o selecionados de forma probabil√≠stica um novo conjunto de valores de hiperpar√¢metros com maior probabilidade de gerar melhores resultados. 
</p>

Finalmente, em rela√ß√£o a essa forma de otimiza√ß√£o, o modo cl√°ssico do m√≥dulo ``Optuna`` foi utilizado:
</p>

![image](https://github.com/user-attachments/assets/ba9da3be-ec97-4f31-a251-2703c011fb9d)


## üòÅ Conclus√£o

<p align="justify">
Ao final do projeto, foi poss√≠vel otimizar os par√¢metros de uma rede MLP classificadora multiclasse, a qual visava a previs√£o da severidade de enfisemas pulmonares em pacientes com HIV. A partir dos tr√™s m√©todos de otimiza√ß√£o apicados, foi poss√≠vel obter tr√™s conjuntos de hiperpar√¢metros. Vale ressaltar que os hiperpar√¢metros dos m√©todos de busca em grade e busca bayesiana foram definidos dentro de um intervalo poss√≠vel, enquanto a busca em grade foi feita com valores de hiperpar√¢metros definidos de forma discreta e sem dropout, sendo que, mesmo assim, a busca em grade apresentou o melhor resultado. A partir do treinamento e teste das redes MLP com cada uma das combina√ß√µes de par√¢metros, obteve-se diferentes valores de acur√°cia, sendo os dois maiores valores de 41.18% com a busca aleat√≥ria e bayesiana. Quanto ao menor resultado, foi a acur√°cia de 35.29%, com a busca em grade. Quanto a matriz de confus√£o tanto a busca em grade, quanto busca aleat√≥ria apresentam apenas dois valores na diagonal principal, enquanto a busca bayesiana apresenta tr√™s. Vale ressaltar tamb√©m que em todas, a predi√ß√µes mais fidedignas foram em rela√ß√£o a classe 0, que apresenta maior frequ√™ncia de dados. Apesar dos valores iguais entre a acur√°cia do modelo Bayesiano e de Busca aleat√≥ria, as matrizes de confus√£o foram diferentes o que demonstra modelos diferentes em suas predi√ß√µes. Al√©m disso,a aplica√ß√£o da ferramenta de parada antecipada √© uma ferramenta que auxilixa na redu√ß√£o do custo computacional. Portanto, a acur√°cia pr√≥xima ao baseline, pode ter ocorrido devido ao desbalanceamento das classes. Desse modo, foi poss√≠vel testar m√©todos diferentes de otimizadores e avaliar o desempenho de cada um deles para a predi√ß√£o desse modelo.
</p>

## üñáÔ∏è Informa√ß√µes t√©cnicas
* Linguagem de programa√ß√£o: `Python 3.9`
* Software:  `Jupyter Notebook`

* Bibliotecas e M√≥dulos: `random` `zipfile` `os` `pandas` `scikit-learn` `skorch` `pandas` `numpy` `itertools` `torch` `optuna` `copy`
<br>


## üë©‚Äçü¶≥ Refer√™ncias
[1] [What Is Optimization Modeling? | IBM](https://www.ibm.com/think/topics/optimization-model). Acesso em: 4 maio 2025.

[2] [What Is Hyperparameter Tuning? | IBM](https://www.ibm.com/think/topics/hyperparameter-tuning). Acesso em: 7 maio 2025.

[3] [SKORCH ‚Äì Quickstart: Grid Search](https://skorch.readthedocs.io/en/stable/user/quickstart.html#grid-search). Acesso em: 3 jun. 2025.

[4] [Quickstart ‚Äî skorch 1.1.0 documentation](https://skorch.readthedocs.io/en/stable/user/quickstart.html#grid-search). Acesso em: 3 jun. 2025.

[5] [Tuning MLP by using Optuna ‚Äì Gist by toshihikoyanase](https://gist.github.com/toshihikoyanase/7e75b054395ec0a6dbb144a300862f60). Acesso em: 28 mai. 2025.

[6] [scikit-learn hyperparameter optimization for MLPClassifier ‚Äì PANJEH (Medium)](https://panjeh.medium.com/scikit-learn-hyperparameter-optimization-for-mlpclassifier-4d670413042b). Acesso em: 21 mai. 2025.

 ## üß† Contribui√ß√µes dos Colaboradores
| GitHub | Contribui√ß√µes |
|:-----|:--------------|
| [J√∫lia Guedes A. dos Santos](https://github.com/JuliaGuedesASantos) | Introdu√ß√£o, otimiza√ß√£o de modelos (bayesiano e busca aleat√≥ria), treinamento final dos modelos, coment√°rios no c√≥digo e READ ME |
| [Lorena Ribeiro Nascimento](https://github.com/lorena881) | Introdu√ß√£o, otimiza√ß√£o de modelos (bayesiano e busca aleat√≥ria), coment√°rios no c√≥digo, READ ME | (https://github.com/MEmilyGomes)
| [Maria Emily Nayla Gomes da Silva](https://github.com/lorena881) | Introdu√ß√£o, otimiza√ß√£o de modelos (busca em grade), READ ME e coment√°rios no c√≥digo |
| [Daniel Roberto Cassar](https://github.com/drcassar) | Orientador |
 
